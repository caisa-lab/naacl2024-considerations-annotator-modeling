{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760ef1c4-8b6e-486a-9e2b-0d5d8d0b9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, BCELoss, BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchmetrics.classification import BinaryF1Score, BinaryPrecision, BinaryRecall, BinaryAccuracy\n",
    "#from torchsummary import summary\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a09ead1-12fd-4633-8476-b0cc5aef347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = 'cuda:2'\n",
    "DEVICE = torch.device(CUDA if torch.cuda.is_available() else 'cpu') \n",
    "path_to_data = '../data/'\n",
    "#model_version = 'sentence-transformers/all-distilroberta-v1'\n",
    "#model_version_mini = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37c359f-aabd-4d42-acfc-e2b835e182b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7984ad-0528-4985-bced-cb7da3872797",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9144e2c5-8833-43e5-bf7f-7d09abb24c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_data+'all_annotators.csv', 'r') as f:\n",
    "    all_df = pd.read_csv(f)\n",
    "\n",
    "with open(path_to_data+'train_all_annotators.csv', 'r') as f:\n",
    "    train_df = pd.read_csv(f)\n",
    "\n",
    "with open(path_to_data+'test_all_annotators.csv', 'r') as f:\n",
    "    test_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158a4981-369a-44c1-bc85-76a69116af8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/GHC/hate_multi.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join( \"data\", 'GHC',\n",
    "                     'hate' + \"_multi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5fd6fd-aec4-4494-b2b2-181589c6ab4f",
   "metadata": {},
   "source": [
    "## create pytorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24440823-0d6c-473b-b56b-b385b502718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe needs to have Columns like this: [Text, Annotator-1_label, ..., Annotator-N_label]\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, annotations_df, min_number_of_annotations, tokenizer, max_length):\n",
    "        super().__init__()\n",
    "        annotations_df = filter_df_min_annotation(annotations_df, min_number_of_annotations)\n",
    "        self.annotator_ids = [x for x in annotations_df.columns if re.fullmatch(r'[0-9]+',x)]\n",
    "        self.num_annotators = len(self.annotator_ids)\n",
    "        \n",
    "        self.texts = annotations_df.text\n",
    "        self.labels = annotations_df[self.annotator_ids]\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        labels = list(self.labels.iloc[idx,:])\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'masks': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "            }\n",
    "\n",
    "\n",
    "def filter_df_min_annotation(df,min_annotations):\n",
    "    annotators = [c for c in df.columns if re.fullmatch(r'[0-9]+',c)]\n",
    "    rest = [x for x in df.columns if x not in annotators]\n",
    "    filter_df = df[annotators]\n",
    "    filter_df = filter_df.replace(-1,float('nan'))\n",
    "    filtered_annotators = [a for a,c in filter_df.count(axis=0).items() if c >= min_annotations]\n",
    "    return df[rest+filtered_annotators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f3990a4-e395-4fd6-b876-bf6fd71d96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = MultiTaskDataset(all_df, 1, tokenizer, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4189cc95-a55f-4878-90fe-8df9c4464ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fa1a8-8f75-4856-a95e-ccc3feb75605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21db3ee5-f43d-4421-8e79-949d370b119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eb70783-613e-4fff-895b-234ec5c625c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': tensor([[  101, 24151,  4665,  ...,     0,     0,     0],\n",
      "        [  101,  3738,  2024,  ...,     0,     0,     0],\n",
      "        [  101,  3539,  2099,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1022,  2442,  ...,     0,     0,     0],\n",
      "        [  101,  1022,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  1030, 19785,  ...,     0,     0,     0]]),\n",
      " 'labels': tensor([[-1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1,  0, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1,  0, -1],\n",
      "        [-1, -1,  0, -1, -1,  0, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,  1,  0, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1,  0, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [-1, -1,  0, -1, -1,  0, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1, -1, -1],\n",
      "        [-1, -1, -1,  0, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1,  0, -1, -1,  0,  0, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1],\n",
      "        [ 0,  0, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1,  0, -1, -1,  0, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1,  0, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1, -1, -1,  0, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,  1,  0, -1, -1, -1, -1, -1]]),\n",
      " 'masks': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/neuendob/anaconda3/envs/master/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i, data_dict in enumerate(dataloader):\n",
    "    pprint(data_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30dd65-dc37-4b99-8156-89b5af390de2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42724378-aad4-49aa-a2f3-e8a571184838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, num_annotators, freeze_bert=False, train_last_bert_k=0, bert_dim=768):\n",
    "        super().__init__()\n",
    "        self.freeze_bert = freeze_bert\n",
    "        self.num_annotators = num_annotators\n",
    "        self.train_last_bert_k = train_last_bert_k\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # this is only needed for freezing bert layers is wanted.\n",
    "        self.bert_modules = nn.ModuleList(self.bert_model.children())[:-1]\n",
    "        self.bert_modules = nn.Sequential(*self.bert_modules)\n",
    "        self.base_number_of_layers = self._get_layer_count(self.bert_modules)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=0.1)\n",
    "        \n",
    "        if freeze_bert:\n",
    "            self.freeze_feature_layers_until_last_k(train_last_bert_k)\n",
    "            \n",
    "        for i in range(self.num_annotators):\n",
    "            setattr(self, f\"fc{i}\", nn.Linear(bert_dim, 2))\n",
    "            \n",
    "        # initialize all fc layers to xavier\n",
    "        # Skip it for now. Is not in the original paper.\n",
    "        #for m in self.modules():\n",
    "        #    if isinstance(m, nn.Linear):\n",
    "        #        torch.nn.init.xavier_normal_(m.weight, gain = 1)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _,bert_out = self.bert_model(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        drop_out = self.dropout_layer(bert_out)\n",
    "        \n",
    "        clf_outputs = {}\n",
    "        for i in range(self.num_annotators):\n",
    "            lin = getattr(self, f\"fc{i}\")(drop_out)\n",
    "            clf_outputs[f\"fc{i}\"] = lin\n",
    "        \n",
    "        return clf_outputs\n",
    "    \n",
    "        \n",
    "    def _set_freeze_(self, status ,k=0): #BERT has 12 layers\n",
    "        last_layer_number = self.base_number_of_layers - k - 1\n",
    "        for n,p in self.bert_modules.named_parameters():\n",
    "            layer_number = self._get_layer_number(n)\n",
    "            if layer_number <= last_layer_number: \n",
    "                p.requires_grad = status\n",
    "\n",
    "    def freeze_feature_layers(self):\n",
    "        self._set_freeze_(False)\n",
    "        \n",
    "    def unfreeze_feature_layers(self):\n",
    "        self._set_freeze_(True)\n",
    "        \n",
    "    def unfreeze_all_layers(self):\n",
    "        self._set_freeze_(True)\n",
    "        for i in range(self.num_annotators):\n",
    "            layer = getattr(self, f\"fc{i}\")\n",
    "            layer.requires_grad = True\n",
    "        \n",
    "    def freeze_feature_layers_until_last_k(self,k):\n",
    "        self.unfreeze_feature_layers()\n",
    "        self._set_freeze_(False, k)\n",
    "    \n",
    "    def size(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def _get_layer_count(self,modules):\n",
    "        numbers = -1\n",
    "        for n,p in modules.named_parameters():\n",
    "            num = self._get_layer_number(n)\n",
    "            if num > numbers:\n",
    "                numbers = num\n",
    "        return numbers+1\n",
    "    \n",
    "    def _get_layer_number(self,name):\n",
    "        m = re.search('layer\\.\\d*\\.',name)\n",
    "        number = 0\n",
    "        if m:\n",
    "            number = m.group().split('.')[-2]\n",
    "            number = int(number)\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7c20af5-60f0-4433-9903-f632c2ba7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    def __init__(self, annotator_weights):\n",
    "        super().__init__()\n",
    "        self.annotator_weights = annotator_weights.values()\n",
    "\n",
    "    def forward(self, preds, true_vals):            \n",
    "        losses = []\n",
    "        for pred, true_val, weight in zip(preds, true_vals, self.annotator_weights):\n",
    "            if true_val == -1:\n",
    "                losses.append(0)\n",
    "            else:\n",
    "                target = F.one_hot(true_val.to(torch.int64),num_classes=2).float().to(DEVICE)\n",
    "                loss = F.binary_cross_entropy_with_logits(\n",
    "                    input=pred.to(DEVICE),\n",
    "                    target=target.to(DEVICE), \n",
    "                    weight=torch.tensor(weight).to(DEVICE))\n",
    "                losses.append(loss)\n",
    "           \n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d5a95d5-9eec-4cbd-8774-b17e76771d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annotator_class_weights(dataframe):\n",
    "    annotator_numbers = [x for x in dataframe.columns if re.fullmatch(r'[0-9]+',x)]\n",
    "    weights = dict()\n",
    "    for i in annotator_numbers:\n",
    "        labels = [x for x in dataframe[str(i)].values if x != -1]\n",
    "        weight = compute_class_weight(\n",
    "              class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "        weights[i] = weight\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca45872c-a599-4131-a24a-e565bafcc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_min_annotation(df,min_annotations):\n",
    "    annotators = [c for c in df.columns if re.fullmatch(r'[0-9]+',c)]\n",
    "    rest = [x for x in df.columns if x not in annotators]\n",
    "    filter_df = df[annotators]\n",
    "    filter_df = filter_df.replace(-1,float('nan'))\n",
    "    filtered_annotators = [a for a,c in filter_df.count(axis=0).items() if c >= min_annotations]\n",
    "    return df[rest+filtered_annotators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b20b40ab-bacf-4eb5-95c0-bfde75572657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, loss_fn, train_only_annotated, print_interval=100):\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    bin_acc = BinaryAccuracy().to(DEVICE)\n",
    "\n",
    "    # using set_grad_enabled() we can enable or disable\n",
    "    # the gardient accumulation and calculation, this is specially\n",
    "    # good for conserving more memory at validation time and higher performance\n",
    "    with torch.set_grad_enabled(True):    \n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i, data_dict in enumerate(tqdm(dataloader)):\n",
    "            preds, labels = _model_and_process(data_dict, model)\n",
    "\n",
    "            #the loss needs to be calculated for each sample, so it is not called on the batch\n",
    "            single_losses = []\n",
    "            for pred_vec, label_vec in zip(preds,labels):\n",
    "                loss=loss_fn(pred_vec,label_vec)\n",
    "                single_losses.append(loss)\n",
    "\n",
    "            loss_batch = torch.stack(single_losses).mean()\n",
    "            losses.append(loss_batch.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_batch.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "            preds_bin_1dim = torch.topk(preds, 1, dim=2, largest=True, sorted=True, out=None)[1].squeeze(dim=2)\n",
    "            \n",
    "            mask = labels.not_equal(-1)\n",
    "            masked_1d_labels = torch.masked_select(labels,mask)\n",
    "            masked_1d_preds = torch.masked_select(preds_bin_1dim,mask)\n",
    "\n",
    "            all_acc = bin_acc(masked_1d_preds,masked_1d_labels)\n",
    "            accs.append(all_acc)\n",
    "\n",
    "            if i%print_interval==0:\n",
    "                print(f'[Training] Itteration/Batch: {i:>3}: Loss: {loss_batch:.2f} | Accuracy: {all_acc:.2f}')\n",
    "                logging.info(f'[Training] Itteration/Batch: {i:>3}: Loss: {loss_batch:.2f} | Accuracy: {all_acc:.2f}')\n",
    "                \n",
    "    return (losses, accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0cfa97-65fc-49e9-8c25-0ea686e8ce58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## test and debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e675df-b41f-413a-a324-638e2ff94b82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### run cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5897e52-31e3-4342-888c-ee7841d62a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_annotators = 18\n",
    "batch_size = 32\n",
    "num_epochs  = 3\n",
    "num_splits = 5\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset= MultiTaskDataset(test_df, num_annotators, tokenizer, max_length=100, is_training_set=True)\n",
    "loss_fn = MultiTaskLossWrapper().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d11f0d9-1a5f-4917-b3aa-e4c320a54f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = KFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8db04c4b-bb3d-40d2-8319-9c6d65f4b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sure you can use cuda:2, that is the second graphic card? If so, resolve this malicious code:\n",
    "break\n",
    "CUDA = 'cuda:2'\n",
    "DEVICE = torch.device(CUDA if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4fef9-41aa-47f6-898d-99235fbe7cf9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = {'train_loss': [], \n",
    "           'test_loss': [],\n",
    "           'train_f1s':[],\n",
    "           'test_f1s':[],\n",
    "           'train_accs':[],\n",
    "           'test_accs':[],\n",
    "           'individual_train_f1s':[], \n",
    "           'individual_test_f1s':[]}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    model = MultiTaskBERT(num_annotators=num_annotators, freeze_bert=False)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(),lr= 1e-7)\n",
    "    \n",
    "    print('#'*30, f'\\t Fold {fold+1} \\t', '#'*30)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('_'*30, f'\\t Running Epoch {epoch+1} of {num_epochs} \\t', '_'*30)\n",
    "        \n",
    "        (tr_prec, tr_rec, tr_f1s, tr_losses, tr_all_f1s, tr_all_accs) = train_val_epoch(model=model, \n",
    "                                                                     num_annotators=num_annotators, \n",
    "                                                                     dataloader=train_loader, \n",
    "                                                                     optimizer=optimizer, \n",
    "                                                                     loss_fn=loss_fn, \n",
    "                                                                     is_training=True, \n",
    "                                                                     train_only_annotated=False)\n",
    "        (te_prec, te_rec, te_f1s, te_losses, te_all_f1s, te_all_accs) = train_val_epoch(model=model, \n",
    "                                                                     num_annotators=num_annotators, \n",
    "                                                                     dataloader=test_loader, \n",
    "                                                                     optimizer=optimizer, \n",
    "                                                                     loss_fn=loss_fn, \n",
    "                                                                     is_training=False, \n",
    "                                                                     train_only_annotated=False)\n",
    "        \n",
    "        train_loss = float(torch.tensor(tr_losses).mean())\n",
    "        train_f1 = float(torch.tensor(tr_all_f1s).mean())\n",
    "        train_acc = float(torch.tensor(tr_all_accs).mean())\n",
    "        test_loss =float(torch.tensor(te_losses).mean())\n",
    "        test_f1 = float(torch.tensor(te_all_f1s).mean())\n",
    "        test_acc = float(torch.tensor(te_all_accs).mean())\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}: AVG Training Loss:{train_loss} AVG Test Loss:{test_loss} AVG Training F1 {train_f1} AVG Test F1 {test_f1}\")\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_f1s'].append(train_f1)\n",
    "        history['test_f1s'].append(test_f1)\n",
    "        history['train_accs'].append(train_acc)\n",
    "        history['test_accs'].append(test_acc)\n",
    "        history['individual_train_f1s'].append(tr_f1s)\n",
    "        history['individual_test_f1s'].append(te_f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183319b-9392-486b-a104-419bdfce515f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### check if only annotated FCs get updated in loss_fn.backward() in train epoch \n",
    "\n",
    "yes, with batchsize = 1 you can see only three FCs have gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8dbc7f25-255e-4699-b0d3-019ce3863122",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\\n       '13', '17'],\\n      dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m filter_df_min_annotation(all_df, min_number_of_annotations)\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultiTaskDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_number_of_annotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m num_annotators \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mnum_annotators\n\u001b[1;32m     17\u001b[0m weights \u001b[38;5;241m=\u001b[39m calc_annotator_class_weights(dataframe)\n",
      "Cell \u001b[0;32mIn [77], line 10\u001b[0m, in \u001b[0;36mMultiTaskDataset.__init__\u001b[0;34m(self, annotations_df, min_number_of_annotations, tokenizer, max_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_annotators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotator_ids)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts \u001b[38;5;241m=\u001b[39m annotations_df\u001b[38;5;241m.\u001b[39mText\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[43mannotations_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotator_ids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;241m=\u001b[39mmax_length\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/pandas/core/indexing.py:1194\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/pandas/core/indexing.py:1132\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1134\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/pandas/core/indexing.py:1330\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1327\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1328\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1330\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/pandas/core/indexes/base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/pandas/core/indexes/base.py:5856\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5855\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\\n       '13', '17'],\\n      dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs  = 10\n",
    "num_splits = 5\n",
    "print_interval = 200/batch_size\n",
    "learning_rate = 5e-5\n",
    "only_one_fold = True\n",
    "max_length = 64\n",
    "min_number_of_annotations = 3000\n",
    "\n",
    "dataframe = filter_df_min_annotation(all_df, min_number_of_annotations)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = MultiTaskDataset(dataframe, min_number_of_annotations, tokenizer, max_length=max_length)\n",
    "\n",
    "num_annotators = dataset.num_annotators\n",
    "\n",
    "weights = calc_annotator_class_weights(dataframe)\n",
    "loss_fn = MultiTaskLossWrapper(annotator_weights=weights).to(DEVICE)\n",
    "\n",
    "CUDA = 'cuda'\n",
    "DEVICE = 'cpu'#torch.device(CUDA if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "model = MultiTaskBERT(num_annotators, freeze_bert=False)\n",
    "model.to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(),lr= 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304acbf-366f-4bdf-9722-65119f72e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(True):    \n",
    "        \n",
    "    model.train()\n",
    "\n",
    "    for i, data_dict in enumerate(dataloader):\n",
    "        token_ids = data_dict['ids'].to(DEVICE) \n",
    "        token_type_ids = data_dict['token_type_ids'].to(DEVICE) \n",
    "        masks = data_dict['masks'].to(DEVICE) \n",
    "        labels = data_dict['labels'].to(DEVICE)\n",
    "\n",
    "        output = model(\n",
    "            ids=token_ids,\n",
    "            mask=masks,\n",
    "            token_type_ids=token_type_ids)   \n",
    "\n",
    "        preds = list(output.values()) #values, because output is a dict, keys are the fc layers of the model \n",
    "        preds = torch.stack(preds)\n",
    "        preds = preds.transpose(0,1) #such that we have shape(n_batch, n_annotators, n_classes=2)\n",
    "\n",
    "        labels = labels.type_as(preds)\n",
    "        \n",
    "        \n",
    "        #the loss needs to be calculated for each sample, so it is not called on the batch\n",
    "        single_losses = []\n",
    "        for pred_vec, label_vec in zip(preds,labels):\n",
    "            loss=loss_fn(pred_vec,label_vec)\n",
    "            single_losses.append(loss)\n",
    "\n",
    "        loss_batch = torch.stack(single_losses).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_batch.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        break\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5f1cfa9-7e09-4867-95ee-ebccd6974abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 15])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b80e7b2e-92df-4021-bfe1-66817e818e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 2])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad722289-d874-4efe-8d20-42f64a1385e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 15])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b240a42-c593-431a-a103-50ba74fa7d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc0.weight tensor(-0.0064)\n",
      "fc1.weight tensor(-1.2619)\n",
      "fc2.weight tensor(-0.7301)\n",
      "fc3.weight tensor(-1.6932)\n",
      "fc5.weight tensor(-0.3889)\n",
      "fc6.weight tensor(-0.6615)\n",
      "fc7.weight tensor(-1.4467)\n",
      "fc11.weight tensor(-0.2987)\n",
      "fc12.weight tensor(-5.1947)\n",
      "fc13.weight tensor(-4.6475)\n",
      "fc16.weight tensor(-2.4237)\n"
     ]
    }
   ],
   "source": [
    "# get names of FC layers with non-zero gradient.\n",
    "affected = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and 'fc' in name:\n",
    "        if param.grad.sum() != 0:\n",
    "            print(name, param.grad.sum())\n",
    "            affected.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71352d-c016-4ba4-b0dd-2911f50cd64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68a6cd68-702e-47f2-ab04-5250fd2323ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f46580a-c270-4447-8f51-e54fa5047659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e994d255523c4ece8e1fd95200b70d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs  = 10\n",
    "num_splits = 5\n",
    "print_interval = 200/batch_size\n",
    "learning_rate = 5e-5\n",
    "only_one_fold = True\n",
    "max_length = 64\n",
    "min_number_of_annotations = 1\n",
    "\n",
    "dataframe = filter_df_min_annotation(all_df, min_number_of_annotations)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = MultiTaskDataset(dataframe, min_number_of_annotations, tokenizer, max_length=max_length)\n",
    "\n",
    "num_annotators = dataset.num_annotators\n",
    "\n",
    "weights = calc_annotator_class_weights(dataframe)\n",
    "loss_fn = MultiTaskLossWrapper(annotator_weights=weights).to(DEVICE)\n",
    "\n",
    "CUDA = 'cuda'\n",
    "DEVICE = 'cpu'#torch.device(CUDA if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "model = MultiTaskBERT(num_annotators, freeze_bert=False)\n",
    "model.to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(),lr= 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65c362cd-d610-4bdb-93c1-47a0ef8bc176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/app/home/neuendob/anaconda3/envs/master/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(True):    \n",
    "        \n",
    "    model.train()\n",
    "\n",
    "    for i, data_dict in enumerate(dataloader):\n",
    "        token_ids = data_dict['ids'].to(DEVICE) \n",
    "        token_type_ids = data_dict['token_type_ids'].to(DEVICE) \n",
    "        masks = data_dict['masks'].to(DEVICE) \n",
    "        labels = data_dict['labels'].to(DEVICE)\n",
    "\n",
    "        output = model(\n",
    "            ids=token_ids,\n",
    "            mask=masks,\n",
    "            token_type_ids=token_type_ids)   \n",
    "\n",
    "        preds = list(output.values()) #values, because output is a dict, keys are the fc layers of the model \n",
    "        preds = torch.stack(preds)\n",
    "        preds = preds.transpose(0,1) #such that we have shape(n_batch, n_annotators, n_classes=2)\n",
    "\n",
    "        labels = labels.type_as(preds)\n",
    "        \n",
    "        \n",
    "        #the loss needs to be calculated for each sample, so it is not called on the batch\n",
    "        single_losses = []\n",
    "        for pred_vec, label_vec in zip(preds,labels):\n",
    "            loss=loss_fn(pred_vec,label_vec)\n",
    "            single_losses.append(loss)\n",
    "\n",
    "        loss_batch = torch.stack(single_losses).mean()\n",
    "        \n",
    "        break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_final.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        # get names of FC layers with non-zero gradient.\n",
    "        affected = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and 'fc' in name:\n",
    "                if param.grad.sum() != 0:\n",
    "                    affected.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dee2712-84a7-4a8a-b0d8-0928cebedda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b36a6a-2d58-4969-b8ba-a754aa9c2ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b355e367-2901-4f35-ab9d-eee52053e43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6777, -0.0925],\n",
       "        [-0.4624,  0.1261],\n",
       "        [-0.6270,  0.2642],\n",
       "        [-0.6609,  0.2400],\n",
       "        [-0.5805,  0.1754],\n",
       "        [-0.3018, -0.0274],\n",
       "        [-0.5309,  0.0210],\n",
       "        [-0.5920,  0.1980],\n",
       "        [-0.5246, -0.0557],\n",
       "        [-0.4550, -0.1275],\n",
       "        [-0.6951,  0.1632],\n",
       "        [-0.5830,  0.2511],\n",
       "        [-0.5432,  0.2115],\n",
       "        [-0.5651,  0.3057],\n",
       "        [-0.1779, -0.0308],\n",
       "        [-0.6336, -0.1782]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = preds[:,0,:]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "902f1eaa-902e-488a-8f93-2cb5509cd726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  0., -1., -1.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e79c0eb6-09a0-4fb9-a147-b69428367373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = labels[:,1] != -1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "95fc031c-4858-41c4-955b-40602cf33317",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_0 = torch.tensor([[3,-1]]*3,dtype=torch.float)\n",
    "test_input_1 = torch.tensor([[-1,3]]*3,dtype=torch.float)\n",
    "test_target_0 = torch.tensor([0]*3,dtype=torch.long)\n",
    "test_target_1 = torch.tensor([1]*3,dtype=torch.long)\n",
    "test_target_0_one_hot = torch.tensor([[1,0]]*3,dtype=torch.float)\n",
    "test_target_1_one_hot = torch.tensor([[0,1]]*3,dtype=torch.float)\n",
    "\n",
    "weight = torch.tensor([0.1,10],dtype=torch.float)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2f9bcc67-e822-45bf-af80-f946f4575fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.1,10],dtype=torch.float).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e9a9db5a-220c-4822-a862-c22a17ee7c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.1815)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(input=test_input_0, \n",
    "        target=test_target_1_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "32cf1797-b826-45ea-889f-704b658428cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4018)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(input=test_input_1, \n",
    "        target=test_target_0_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "0cacdcab-9764-426c-bdab-2b1dd0d967f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0181)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(input=test_input_0, \n",
    "        target=test_target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "8bf6b86f-7831-405d-ae3f-0908e2212da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0181)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(input=test_input_1, \n",
    "        target=test_target_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d7708ceb-c8f3-460a-b272-5e56fc8fd415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  0., -1.,  0.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = labels[:,0]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c468eabe-0fe6-401f-ae48-51685f4c362e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_f = torch.stack([x for x in l if x != -1])\n",
    "l_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "85ea5efa-e070-4c1c-b5aa-ff9df3f126dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf2 = F.one_hot(l_f.to(torch.int64),num_classes=2).float()\n",
    "lf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47b4e2f8-77e0-4f56-a56c-06e4ab1cb1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4624,  0.1261],\n",
       "        [-0.5830,  0.2511],\n",
       "        [-0.5651,  0.3057]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_f = torch.stack(\n",
    "    [x for x,y in zip(p,l) if y != -1])\n",
    "p_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "88399392-4a5e-47c6-903e-9b852f85082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1148, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = labels[:,1] != -1\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn(input=p_f,\n",
    "        target=lf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e563b060-f6bc-41a2-bab0-3546c3dc8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(\n",
    "                weight=torch.tensor([0.9,1.5],dtype=torch.float))\n",
    "            \n",
    "annos_preds = preds[:,0,:]\n",
    "annos_trues = labels[:,0]\n",
    "\n",
    "annos_preds_filtered = torch.stack(\n",
    "    [pred for pred,true_val in zip(annos_preds,annos_trues) if true_val != -1])\n",
    "annos_lables_filtered = torch.stack(\n",
    "    [true_val for true_val in annos_trues if true_val != -1])\n",
    "\n",
    "annos_lables_filtered_one_hot = F.one_hot(annos_lables_filtered.to(torch.int64),num_classes=2).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5a81ecd4-d835-4423-b389-da9fded2f636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0336, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(input=annos_preds_filtered,\n",
    "        target=annos_lables_filtered_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e2afa-9e6f-4475-ae79-e120245e6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy_with_logits(\n",
    "                        input=preds[:,1,:],\n",
    "                        target=F.one_hot(labels[:,1].to(torch.int64),num_classes=2).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c12d0cb4-4644-4882-873a-47ca74025521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper_old(nn.Module):\n",
    "    \"\"\"The loss needs to be calculated for each sample because each sample has possibly different annotators and thus different pos_sample weights\n",
    "    \"\"\"\n",
    "    def __init__(self, annotator_weights, sum_not_mean=False):\n",
    "        super().__init__()\n",
    "        self.annotator_weights = annotator_weights.values()\n",
    "        self.num_annotators = len(self.annotator_weights)\n",
    "        self.sum_not_mean = sum_not_mean\n",
    "\n",
    "    def forward(self, batch_preds, batch_true_vals):  \n",
    "        sample_losses = []\n",
    "        \n",
    "        # for each sample in the batch\n",
    "        for pred_vec, label_vec in zip(batch_preds, batch_true_vals):\n",
    "            annotator_sample_losses = []\n",
    "            \n",
    "            #for each annotator\n",
    "            for pred, true_val, weight in zip(pred_vec, label_vec, self.annotator_weights):\n",
    "                if true_val == -1:\n",
    "                    annotator_sample_losses.append(0)\n",
    "                else:\n",
    "                    target = F.one_hot(true_val.to(torch.int64),num_classes=2).float().to(DEVICE)\n",
    "                    loss = F.binary_cross_entropy_with_logits(\n",
    "                        input=pred.to(DEVICE),\n",
    "                        target=target.to(DEVICE), \n",
    "                        pos_weight=torch.tensor(weight).to(DEVICE))\n",
    "                    annotator_sample_losses.append(loss)\n",
    "\n",
    "            sample_loss = sum(annotator_sample_losses)\n",
    "            sample_losses.append(sample_loss)\n",
    "        \n",
    "        if self.sum_not_mean:\n",
    "            batch_loss = torch.stack(sample_losses).sum()\n",
    "        else:\n",
    "            batch_loss = torch.stack(sample_losses).mean()\n",
    "            \n",
    "        return batch_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    \"\"\"The loss needs to be calculated for each sample because each sample has possibly different annotators and thus different pos_sample weights\n",
    "    \"\"\"\n",
    "    def __init__(self, annotator_weights, sum_not_mean=False):\n",
    "        super().__init__()\n",
    "        self.annotator_weights = annotator_weights.values()\n",
    "        self.num_annotators = len(self.annotator_weights)\n",
    "        self.sum_not_mean = sum_not_mean\n",
    "\n",
    "    def forward(self, batch_preds, batch_true_vals):  \n",
    "        anno_losses = []\n",
    "        \n",
    "        # for each annotator\n",
    "        for anno in range(self.num_annotators):\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=self.annotator_weights[anno])\n",
    "            \n",
    "            annos_preds = batch_preds[:,anno,:]\n",
    "            annos_trues = batch_true_vals[:,anno]\n",
    "            \n",
    "            annos_preds_filtered = torch.stack(\n",
    "                [pred for pred,true_val in zip(annos_preds,annos_trues) if true_val != -1])\n",
    "            annos_preds_filtered = torch.stack(\n",
    "                [true_val for true_val in annos_trues if true_val != -1])\n",
    "            \n",
    "            annos_preds_filtered_one_hot = F.one_hot(annos_preds_filtered.to(torch.int64),num_classes=2).float()\n",
    "            \n",
    "            anno_loss = loss_fn(input=annos_preds_filtered.to(DEVICE),\n",
    "                                target=annos_preds_filtered_one_hot.to(DEVICE))\n",
    "            \n",
    "            anno_losses.append(anno_loss)\n",
    "            \n",
    "        \n",
    "        if self.sum_not_mean:\n",
    "            batch_loss = torch.stack(anno_losses).sum()\n",
    "        else:\n",
    "            batch_loss = torch.stack(anno_losses).mean()\n",
    "        \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd3f2702-5cdc-461b-b55d-9998fabd009e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(preds, 1, dim=2, largest=True, sorted=True, out=None)[1].squeeze(dim=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782315c-e9a0-411d-a4ad-64e84c7ac2da",
   "metadata": {},
   "source": [
    "### class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a429a4e-6dac-491c-85d1-d6ca97be86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: 0: 3855    1: 129\n",
      "Weights:  [ 0.51673152 15.44186047]\n"
     ]
    }
   ],
   "source": [
    "# example class weight, for annotator 1 (annotator 0 is weirdly quite balanced)\n",
    "annotator_id = '1'\n",
    "x = [a for a in all_df[annotator_id] if a != -1]\n",
    "print('Counts:','0:', x.count(0),'   1:', x.count(1))\n",
    "\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(x), y=x)\n",
    "print('Weights: ', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cbb7cd7-c0a4-437b-9c9a-2b941a593a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0 = torch.tensor([[3,-1],[2,-2]],dtype=torch.float)\n",
    "pred_1 = torch.tensor([[-1,3], [-2,2]],dtype=torch.float)\n",
    "\n",
    "true_0 = torch.tensor([[1,0]]*2,dtype=torch.float)\n",
    "true_1 = torch.tensor([[0,1]]*2,dtype=torch.float)\n",
    "\n",
    "weights_tensor = torch.tensor(weights,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "167af3db-9d51-4c52-a44b-4fd8132b5133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1327)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 0 is predicted and also true \n",
    "F.binary_cross_entropy_with_logits(input=pred_0, target=true_0, pos_weight=weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f219f2a4-8ba5-4c53-a87e-5eee936731af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7876)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 1 is predicted and also true \n",
    "F.binary_cross_entropy_with_logits(input=pred_1, target=true_1, pos_weight=weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f26c4b2a-b1b3-4ba1-9ae3-8bbfdc750241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.5746)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 0 is predicted but 1 is true\n",
    "F.binary_cross_entropy_with_logits(input=pred_0, target=true_1, pos_weight=weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7e43110-3e3e-48f0-b0c9-21216e495044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7383)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 1 is predicted but 0 is true\n",
    "F.binary_cross_entropy_with_logits(input=pred_1, target=true_0, pos_weight=weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20f05f9f-9ecc-4c88-b42f-4a58b24ac994",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/functional.py:3160\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([2]))"
     ]
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(\n",
    "    input=torch.tensor([3,-1],dtype=torch.float), \n",
    "    target=torch.tensor([1],dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "aedab0eb-211c-459e-b30c-8523dda14532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6848)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "loss_fn(input=test_input_0, \n",
    "    target=torch.tensor([1,1,0],dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6d73c8f1-5cb3-44ed-840c-5cf4bfe89262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6848)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "loss_fn(input=torch.tensor([[3,-1]]*5,dtype=torch.float), \n",
    "    target=torch.tensor([1,-1,1,0,-1],dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c384c415-c869-4d52-abd6-414e04e41700",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [3, 2], got [3, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[296], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [3, 2], got [3, 5]"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn(input=torch.tensor([[[3,-1]]*5]*3,dtype=torch.float), \n",
    "    target=torch.tensor([[1]*5]*3,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2946dda4-4035-4172-85f9-72f0c3cd32ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[3,-1]]*5]*3,dtype=torch.float).transpose(2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1bd890b5-33cf-4fca-a9a8-2661ae3b6c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1]*5]*3,dtype=torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "52697479-5f3c-4f2f-a832-c94a44111ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0181)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(input=torch.tensor([[3,-1]]*5,dtype=torch.float), \n",
    "    target=torch.tensor([[0,1]]*5,dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6419ea20-abee-47ac-b22c-b7c78056eccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.32579999999999"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.0181*18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "72088eed-c611-4652-9d35-a55119c43a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.47403001418581"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26.0133/4.0181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f712ea26-693b-4c97-8b3d-7ea77d9556fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 18, 2])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[3,-1]]*18]*4,dtype=torch.float).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c535b6d1-3dcc-4b23-ac44-ef3c1199b214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1]*18]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c0866a1e-3f62-4b0a-ab8b-55caa6a2f08a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 2], got [4, 18]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 2], got [4, 18]"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(input=torch.tensor([[[3,-1]]*18]*4,dtype=torch.float), \n",
    "    target=torch.tensor([[1]*18]*4,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ca6c-a7bc-4926-9c7b-061087272feb",
   "metadata": {},
   "source": [
    "### number of annotations per annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "753d031e-2763-4d2e-b1e3-df9232efa7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      7708\n",
       "1      3984\n",
       "2      4941\n",
       "3      4511\n",
       "4      3364\n",
       "5      3776\n",
       "6      4060\n",
       "7      8454\n",
       "8      3305\n",
       "9      3548\n",
       "10     3407\n",
       "11    12650\n",
       "12     8009\n",
       "13     8861\n",
       "14      286\n",
       "15     1384\n",
       "16      542\n",
       "17     3518\n",
       "dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.iloc[:,1:20].replace(0,1).replace(-1,0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9181b-bad4-4fb3-bc04-57adcc1719d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2d4eb95-4952-42c5-bfb7-ebd34e286e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c2f8fc3-0d8e-4c0e-84bf-d8664bbe055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = skf.split(X=all_df.Text, y=all_df.majority_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b445c-34af-48ab-b71e-899b2df05187",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ed44a-ea85-4184-9f4b-744247b3beb4",
   "metadata": {},
   "source": [
    "### other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0a629-faae-42f2-8803-a3731ed478b9",
   "metadata": {},
   "source": [
    "### other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
